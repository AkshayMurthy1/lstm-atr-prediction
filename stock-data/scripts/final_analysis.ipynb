{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtest_1023 Metric\n",
      "ATR                0.676129\n",
      "Squared_Returns    1.346297\n",
      "Name: Relative_MSE, dtype: float64\n",
      "Runtest_1024 Metric\n",
      "ATR                6.213594\n",
      "Squared_Returns    7.287342\n",
      "Name: Relative_MSE, dtype: float64\n",
      "Runtest_1025 Metric\n",
      "ATR                0.288375\n",
      "Squared_Returns    2.595777\n",
      "Name: Relative_MSE, dtype: float64\n",
      "Runtest_1026 Metric\n",
      "ATR                2.382405\n",
      "Squared_Returns    3.144229\n",
      "Name: Relative_MSE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Analyze different time periods for RNN\n",
    "for i in range(1023,1027):\n",
    "    df = pd.read_csv(f'../output/runtest_{i}/overall_results.csv')\n",
    "    print(f\"Runtest_{i}\",df.groupby(\"Metric\")[\"Relative_MSE\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ATR consistently outperforms squared returns, doing best when there is ample amounts of training data. This might demonstrate ATR's ability to not overfit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 Metric\n",
      "ATR                0.830029\n",
      "Squared_Returns    0.895952\n",
      "Name: Relative_MSE, dtype: float64\n",
      "33 Metric\n",
      "ATR                0.771406\n",
      "Squared_Returns    0.628715\n",
      "Name: Relative_MSE, dtype: float64\n",
      "34 Metric\n",
      "ATR                0.799177\n",
      "Squared_Returns    0.824391\n",
      "Name: Relative_MSE, dtype: float64\n",
      "35 Metric\n",
      "ATR                0.724802\n",
      "Squared_Returns    0.823542\n",
      "Name: Relative_MSE, dtype: float64\n",
      "36 Metric\n",
      "ATR                0.794125\n",
      "Squared_Returns    0.964812\n",
      "Name: Relative_MSE, dtype: float64\n",
      "37 Metric\n",
      "ATR                0.680897\n",
      "Squared_Returns    0.544277\n",
      "Name: Relative_MSE, dtype: float64\n",
      "38 Metric\n",
      "ATR                0.751495\n",
      "Squared_Returns    0.594292\n",
      "Name: Relative_MSE, dtype: float64\n",
      "39 Metric\n",
      "ATR                0.628070\n",
      "Squared_Returns    0.565187\n",
      "Name: Relative_MSE, dtype: float64\n",
      "40 Metric\n",
      "ATR                0.751546\n",
      "Squared_Returns    0.677466\n",
      "Name: Relative_MSE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Analyze diff layer combos LSTM\n",
    "for i in range(32,41):\n",
    "    df = pd.read_csv(f'../output/runtest_{i}/overall_results.csv')\n",
    "    print(i,df.groupby('Metric')['Relative_MSE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
